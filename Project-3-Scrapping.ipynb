{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2434a1",
   "metadata": {},
   "source": [
    "# Project 3: Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfcdb61",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "\n",
    "You're fresh out of your Data Science bootcamp and looking to break through in the world of freelance data journalism. Nate Silver and co. at FiveThirtyEight have agreed to hear your pitch for a story in two weeks!\n",
    "\n",
    "Your piece is going to be on how to create a Reddit post that will get the most engagement from Reddit users. Because this is FiveThirtyEight, you're going to have to get data and analyze it in order to make a compelling narrative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ffc6e",
   "metadata": {},
   "source": [
    "### Problem Statement: What characteristics of a post on Reddit are most predictive of the overall interaction on a thread (as measured by number of comments) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035d098",
   "metadata": {},
   "source": [
    "GET:\n",
    "- [x] The title of the thread\n",
    "- [x] The subreddit that the thread corresponds to\n",
    "- [x] The length of time it has been up on Reddit\n",
    "- [x] The number of comments on the thread\n",
    "- [x] Scrape at least 10,000 threads.\n",
    "\n",
    "Once you've got the data, you will build a classification model that, using the text and any other relevant features, predicts whether or not a given Reddit post will have above or below the _median_ number of comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f237158",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14366ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from time import sleep\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4985c08",
   "metadata": {},
   "source": [
    "### Scrapping Data Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea37effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Reddit Scrapper\n",
    "reddit = praw.Reddit(client_id='tTV90taLsVas5YYvv6XiNw', \n",
    "                     client_secret='AidES-QOZHurdZG9U4hVmr6xjxUxJA', \n",
    "                     user_agent='Michael_Capparelli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c542a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Scrape, Reddit FrontPage, Hot Posts Now\n",
    "posts = []\n",
    "r_all = reddit.subreddit('all')\n",
    "for post in r_all.hot(limit=10_000):\n",
    "    posts.append([post.subreddit,post.title,\n",
    "                  post.score,post.num_comments,\n",
    "                  post.created])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['sub','title','score',\n",
    "                                    'comments','created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb89c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf93160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving first scrape to csv\n",
    "reddit.to_csv('data/reddit_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189a92b",
   "metadata": {},
   "source": [
    "https://praw.readthedocs.io/en/stable/code_overview/models/subreddit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d42960",
   "metadata": {},
   "source": [
    "### Scrapping Data Round 2\n",
    "- The first round of scrapping had extreme outliers.\n",
    "- Because the scrapping was done on only recently hot posts, we have outliers that shouldn't be considered hot, such as posts with 10 comments, so we need more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping all top posts, all time\n",
    "posts = []\n",
    "r_all = reddit.subreddit('all')\n",
    "for post in r_all.top(time_filter='all',limit=10_500):\n",
    "    posts.append([post.subreddit,post.title,\n",
    "                  post.score,post.num_comments,\n",
    "                  post.created])\n",
    "    \n",
    "posts = pd.DataFrame(posts,columns=['sub','title','score',\n",
    "                                    'comments','created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a57dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv('data/r_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c999a",
   "metadata": {},
   "source": [
    "### Merging Two Scrapes and Eliminating Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bc74903",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit1 = pd.read_csv('data/reddit_all.csv')\n",
    "reddit2 = pd.read_csv('data/r_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6f7d962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7332, 5), (2410, 5))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit1.shape,reddit2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "000bdaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit3 = pd.concat([reddit1,reddit2]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3efcc343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9708, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e0b8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit3.to_csv('data/reddit_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a1c0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit3 = pd.read_csv('data/reddit_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd241c1e",
   "metadata": {},
   "source": [
    "### Scrapping Data Round 3\n",
    "- Insufficient amount of posts and the outliers are likely not gone, we will grab hot posts from all again, additionally we will increase the limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86fb546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = []\n",
    "r4 = reddit.subreddit('all')\n",
    "for post in r4.hot(limit=10_500):\n",
    "    submissions.append([post.subreddit,post.title,\n",
    "                  post.score,post.num_comments,\n",
    "                  post.created])\n",
    "    \n",
    "submissions = pd.DataFrame(submissions,columns=['sub','title','score',\n",
    "                                          'comments','created'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa1a8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.to_csv('data/reddit_4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf3581",
   "metadata": {},
   "source": [
    "### Concat For Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "04988464",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit4 = pd.read_csv('data/reddit_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdd905e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9708, 5), (7208, 5))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit3.shape, reddit4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "603f5427",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([reddit3,reddit4]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f6ce2e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting unix time to date time\n",
    "df.created = pd.to_datetime(df['created'],unit='s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8265f",
   "metadata": {},
   "source": [
    "### Checking to see if conversion worked, have varying dates, and that duplicates were dropped successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee04150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>Times Square right now</td>\n",
       "      <td>467439</td>\n",
       "      <td>2021-01-30 18:00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>Iâ€™ve found a few funny memories during lockdow...</td>\n",
       "      <td>438832</td>\n",
       "      <td>2020-06-17 16:17:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7301</th>\n",
       "      <td>The Senate. Upvote this so that people see it ...</td>\n",
       "      <td>407258</td>\n",
       "      <td>2017-04-01 12:57:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7304</th>\n",
       "      <td>A short story</td>\n",
       "      <td>369073</td>\n",
       "      <td>2020-06-07 16:27:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>Joe Biden elected president of the United States</td>\n",
       "      <td>365121</td>\n",
       "      <td>2020-11-07 16:28:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12720</th>\n",
       "      <td>Released a demo of my brand new game, PokÃ©mon:...</td>\n",
       "      <td>68</td>\n",
       "      <td>2022-09-01 19:48:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14517</th>\n",
       "      <td>[Self-timer] Should I keep this dress or retur...</td>\n",
       "      <td>66</td>\n",
       "      <td>2022-09-01 19:48:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15749</th>\n",
       "      <td>Chibi Anastasia and Viy are looking very styli...</td>\n",
       "      <td>65</td>\n",
       "      <td>2022-09-01 19:48:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16489</th>\n",
       "      <td>Nottingham Forest are in take to hijack Michy ...</td>\n",
       "      <td>61</td>\n",
       "      <td>2022-09-01 19:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>Valve, come on save him. Wykrhm Reddy where ar...</td>\n",
       "      <td>61</td>\n",
       "      <td>2022-09-01 20:19:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16900 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title   score  \\\n",
       "7299                              Times Square right now  467439   \n",
       "7298   Iâ€™ve found a few funny memories during lockdow...  438832   \n",
       "7301   The Senate. Upvote this so that people see it ...  407258   \n",
       "7304                                       A short story  369073   \n",
       "7300    Joe Biden elected president of the United States  365121   \n",
       "...                                                  ...     ...   \n",
       "12720  Released a demo of my brand new game, PokÃ©mon:...      68   \n",
       "14517  [Self-timer] Should I keep this dress or retur...      66   \n",
       "15749  Chibi Anastasia and Viy are looking very styli...      65   \n",
       "16489  Nottingham Forest are in take to hijack Michy ...      61   \n",
       "16137  Valve, come on save him. Wykrhm Reddy where ar...      61   \n",
       "\n",
       "                  created  \n",
       "7299  2021-01-30 18:00:38  \n",
       "7298  2020-06-17 16:17:27  \n",
       "7301  2017-04-01 12:57:54  \n",
       "7304  2020-06-07 16:27:35  \n",
       "7300  2020-11-07 16:28:37  \n",
       "...                   ...  \n",
       "12720 2022-09-01 19:48:33  \n",
       "14517 2022-09-01 19:48:12  \n",
       "15749 2022-09-01 19:48:01  \n",
       "16489 2022-09-01 19:49:12  \n",
       "16137 2022-09-01 20:19:46  \n",
       "\n",
       "[16900 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['title','score','created']].sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1904a3d",
   "metadata": {},
   "source": [
    "### Our top titles are different along with their scores, we will save the df to a csv and work in a clean notebook to avoid making errors on any scrapping that was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47b622c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/reddit_complete.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3e986f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeopardsAteMyFace</td>\n",
       "      <td>Did they really think he'd pay out?</td>\n",
       "      <td>20509</td>\n",
       "      <td>1031</td>\n",
       "      <td>2022-08-29 20:28:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news</td>\n",
       "      <td>China drought causes Yangtze to dry up, sparki...</td>\n",
       "      <td>19660</td>\n",
       "      <td>1546</td>\n",
       "      <td>2022-08-29 19:15:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MadeMeSmile</td>\n",
       "      <td>He did it!</td>\n",
       "      <td>86623</td>\n",
       "      <td>871</td>\n",
       "      <td>2022-08-29 18:46:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aww</td>\n",
       "      <td>Loving the water.</td>\n",
       "      <td>13717</td>\n",
       "      <td>112</td>\n",
       "      <td>2022-08-29 19:47:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>Biden vows to crack down on colleges 'jacking ...</td>\n",
       "      <td>44057</td>\n",
       "      <td>2259</td>\n",
       "      <td>2022-08-29 18:28:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16895</th>\n",
       "      <td>Superstonk</td>\n",
       "      <td>Did the thing, better late than never! +84</td>\n",
       "      <td>1197</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-09-01 05:10:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16896</th>\n",
       "      <td>Superstonk</td>\n",
       "      <td>Lucy Komisar interview Robert Shapiro on the S...</td>\n",
       "      <td>122</td>\n",
       "      <td>16</td>\n",
       "      <td>2022-09-01 16:41:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16897</th>\n",
       "      <td>Superstonk</td>\n",
       "      <td>It do be like that these days!</td>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-09-01 16:11:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16898</th>\n",
       "      <td>Superstonk</td>\n",
       "      <td>How many times can I keep doing this? ðŸ§± ðŸ§± ðŸ§±</td>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-09-01 18:28:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16899</th>\n",
       "      <td>Superstonk</td>\n",
       "      <td>You must be the change you wish to see in the ...</td>\n",
       "      <td>99</td>\n",
       "      <td>13</td>\n",
       "      <td>2022-09-01 18:10:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16900 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sub                                              title  \\\n",
       "0      LeopardsAteMyFace                Did they really think he'd pay out?   \n",
       "1                   news  China drought causes Yangtze to dry up, sparki...   \n",
       "2            MadeMeSmile                                         He did it!   \n",
       "3                    aww                                  Loving the water.   \n",
       "4               politics  Biden vows to crack down on colleges 'jacking ...   \n",
       "...                  ...                                                ...   \n",
       "16895         Superstonk         Did the thing, better late than never! +84   \n",
       "16896         Superstonk  Lucy Komisar interview Robert Shapiro on the S...   \n",
       "16897         Superstonk                     It do be like that these days!   \n",
       "16898         Superstonk        How many times can I keep doing this? ðŸ§± ðŸ§± ðŸ§±   \n",
       "16899         Superstonk  You must be the change you wish to see in the ...   \n",
       "\n",
       "       score  comments              created  \n",
       "0      20509      1031  2022-08-29 20:28:53  \n",
       "1      19660      1546  2022-08-29 19:15:21  \n",
       "2      86623       871  2022-08-29 18:46:11  \n",
       "3      13717       112  2022-08-29 19:47:36  \n",
       "4      44057      2259  2022-08-29 18:28:16  \n",
       "...      ...       ...                  ...  \n",
       "16895   1197         6  2022-09-01 05:10:22  \n",
       "16896    122        16  2022-09-01 16:41:25  \n",
       "16897    133         4  2022-09-01 16:11:28  \n",
       "16898     89         3  2022-09-01 18:28:09  \n",
       "16899     99        13  2022-09-01 18:10:19  \n",
       "\n",
       "[16900 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/reddit_complete.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
